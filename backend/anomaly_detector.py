# anomaly_detector.py - Zaawansowany system detekcji anomalii na RTG
"""
System detekcji anomalii poprzez por√≥wnywanie obraz√≥w RTG:
1. Znajduje najbardziej podobny obraz wzorcowy (czysty)
2. Wyr√≥wnuje obrazy (image alignment)
3. Oblicza r√≥≈ºnice i wykrywa anomalie
4. Generuje szczeg√≥≈Çowy raport z wizualizacjƒÖ
"""

import os
import cv2
import numpy as np
from pathlib import Path
from typing import List, Tuple, Dict, Optional
import json
from datetime import datetime
from scipy import ndimage
from skimage.metrics import structural_similarity as ssim
from skimage.feature import match_template
import pickle


class ImageMatcher:
    """Znajduje najbardziej podobny obraz wzorcowy"""
    
    def __init__(self, reference_dir: str, processed_dir: str = None):
        """
        Args:
            reference_dir: Katalog z obrazami wzorcowymi (czystymi)
            processed_dir: Dodatkowy katalog z przetworzonymi obrazami (opcjonalny)
        """
        self.reference_dir = Path(reference_dir)
        self.processed_dir = Path(processed_dir) if processed_dir else None
        self.reference_images = []
        self.reference_features = []
        self._load_references()
    
    def _load_references(self):
        """≈Åaduje wszystkie obrazy wzorcowe z g≈Ç√≥wnego i dodatkowego katalogu"""
        print(f"üìÅ ≈Åadowanie obraz√≥w wzorcowych z: {self.reference_dir}")
        
        # Lista katalog√≥w do przeszukania
        directories_to_search = [self.reference_dir]
        
        # Dodaj katalog z przetworzonymi obrazami je≈õli istnieje
        if self.processed_dir and self.processed_dir.exists():
            directories_to_search.append(self.processed_dir)
            print(f"üìÅ Dodatkowo przeszukujƒô: {self.processed_dir}")
        
        # Przeszukaj wszystkie katalogi
        for search_dir in directories_to_search:
            for root, dirs, files in os.walk(search_dir):
                for file in files:
                    # Akceptuj .bmp i inne formaty obraz√≥w, ignoruj pliki z 'czarno' w nazwie
                    if (file.lower().endswith(('.bmp', '.jpg', '.jpeg', '.png')) and 
                        'czarno' not in file.lower()):
                        
                        img_path = Path(root) / file
                        img = cv2.imread(str(img_path), cv2.IMREAD_GRAYSCALE)
                        if img is not None:
                            # Oblicz features dla szybszego dopasowywania
                            features = self._extract_features(img)
                            self.reference_images.append({
                                'path': img_path,
                                'image': img,
                                'features': features,
                                'source': 'processed' if search_dir == self.processed_dir else 'original'
                            })
        
        print(f"‚úÖ Za≈Çadowano {len(self.reference_images)} obraz√≥w wzorcowych")
        
        # Podsumowanie ≈∫r√≥de≈Ç
        original_count = sum(1 for img in self.reference_images if img['source'] == 'original')
        processed_count = sum(1 for img in self.reference_images if img['source'] == 'processed')
        print(f"   üìÇ Oryginalne: {original_count}")
        print(f"   üîß Przetworzone: {processed_count}")
    
    def _extract_features(self, img: np.ndarray) -> Dict:
        """Wyodrƒôbnia cechy obrazu do por√≥wnywania"""
        # Zmniejsz dla szybszego przetwarzania
        small = cv2.resize(img, (256, 256))
        
        # Histogram
        hist = cv2.calcHist([small], [0], None, [64], [0, 256])
        hist = cv2.normalize(hist, hist).flatten()
        
        # Gradient magnitude (krawƒôdzie)
        sobelx = cv2.Sobel(small, cv2.CV_64F, 1, 0, ksize=3)
        sobely = cv2.Sobel(small, cv2.CV_64F, 0, 1, ksize=3)
        gradient_mag = np.sqrt(sobelx**2 + sobely**2)
        
        # Momenty obrazu
        moments = cv2.moments(small)
        
        return {
            'histogram': hist,
            'gradient_mean': np.mean(gradient_mag),
            'gradient_std': np.std(gradient_mag),
            'intensity_mean': np.mean(small),
            'intensity_std': np.std(small),
            'moments': moments
        }
    
    def _compare_features(self, features1: Dict, features2: Dict) -> float:
        """Por√≥wnuje cechy dw√≥ch obraz√≥w, zwraca podobie≈Ñstwo [0-1]"""
        # Histogram correlation
        hist_corr = cv2.compareHist(
            features1['histogram'], 
            features2['histogram'], 
            cv2.HISTCMP_CORREL
        )
        
        # Statystyki gradientu
        grad_diff = abs(features1['gradient_mean'] - features2['gradient_mean'])
        grad_diff_norm = 1 - min(grad_diff / 255.0, 1.0)
        
        # Statystyki intensywno≈õci
        int_diff = abs(features1['intensity_mean'] - features2['intensity_mean'])
        int_diff_norm = 1 - min(int_diff / 255.0, 1.0)
        
        # ≈ÅƒÖczone podobie≈Ñstwo (wa≈ºona ≈õrednia)
        similarity = (
            0.5 * hist_corr +
            0.25 * grad_diff_norm +
            0.25 * int_diff_norm
        )
        
        return max(0, min(1, similarity))
    
    def find_best_match(self, query_img: np.ndarray, top_k: int = 5) -> List[Dict]:
        """
        Znajd≈∫ najbardziej podobne obrazy wzorcowe
        
        Args:
            query_img: Obraz do dopasowania
            top_k: Ile najlepszych dopasowa≈Ñ zwr√≥ciƒá
            
        Returns:
            Lista s≈Çownik√≥w z informacjami o dopasowaniach
        """
        query_features = self._extract_features(query_img)
        
        matches = []
        for ref in self.reference_images:
            similarity = self._compare_features(query_features, ref['features'])
            matches.append({
                'path': ref['path'],
                'image': ref['image'],
                'similarity': similarity
            })
        
        # Sortuj po podobie≈Ñstwie
        matches.sort(key=lambda x: x['similarity'], reverse=True)
        
        return matches[:top_k]


class ImageAligner:
    """Wyr√≥wnuje dwa obrazy dla dok≈Çadnego por√≥wnania"""
    
    @staticmethod
    def align_images(reference: np.ndarray, image: np.ndarray, 
                     method: str = 'ecc') -> Tuple[np.ndarray, np.ndarray]:
        """
        Wyr√≥wnuje obraz do referencji
        
        Args:
            reference: Obraz wzorcowy
            image: Obraz do wyr√≥wnania
            method: 'ecc' lub 'feature' (Enhanced Correlation Coefficient)
            
        Returns:
            (aligned_image, transformation_matrix)
        """
        # Upewnij siƒô, ≈ºe obrazy majƒÖ ten sam rozmiar
        if reference.shape != image.shape:
            image = cv2.resize(image, (reference.shape[1], reference.shape[0]))
        
        if method == 'ecc':
            return ImageAligner._align_ecc(reference, image)
        else:
            return ImageAligner._align_feature(reference, image)
    
    @staticmethod
    def _align_ecc(reference: np.ndarray, image: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """Wyr√≥wnanie ECC (Enhanced Correlation Coefficient)"""
        print("üîß Rozpoczynanie wyr√≥wnywania ECC...")
        
        # Konwertuj do float32
        ref_gray = reference.astype(np.float32)
        img_gray = image.astype(np.float32)
        
        # Zdefiniuj typ transformacji (affine)
        warp_mode = cv2.MOTION_AFFINE
        warp_matrix = np.eye(2, 3, dtype=np.float32)
        
        # Kryteria zako≈Ñczenia - bardziej liberalne dla szybko≈õci
        criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 1000, 1e-4)
        
        try:
            print("üîÑ Wykonywanie findTransformECC...")
            # Wyr√≥wnaj obrazy z timeout
            _, warp_matrix = cv2.findTransformECC(
                ref_gray, img_gray, warp_matrix, warp_mode, criteria, 
                inputMask=None, gaussFiltSize=3
            )
            
            print("‚úÖ ECC zako≈Ñczone pomy≈õlnie")
            # Zastosuj transformacjƒô
            aligned = cv2.warpAffine(
                image, warp_matrix, (reference.shape[1], reference.shape[0]),
                flags=cv2.INTER_LINEAR + cv2.WARP_INVERSE_MAP
            )
            
            return aligned, warp_matrix
            
        except Exception as e:
            print(f"‚ö†Ô∏è Wyr√≥wnanie ECC nie powiod≈Ço siƒô: {e}, zwracam oryginalny obraz")
            # Fallback - prosta korekta rozmiaru
            if reference.shape != image.shape:
                aligned = cv2.resize(image, (reference.shape[1], reference.shape[0]))
            else:
                aligned = image.copy()
            return aligned, np.eye(2, 3, dtype=np.float32)
    
    @staticmethod
    def _align_feature(reference: np.ndarray, image: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """Wyr√≥wnanie oparte na cechach (ORB)"""
        # Wykryj punkty kluczowe i deskryptory
        orb = cv2.ORB_create(5000)
        kp1, des1 = orb.detectAndCompute(reference, None)
        kp2, des2 = orb.detectAndCompute(image, None)
        
        if des1 is None or des2 is None:
            return image, np.eye(2, 3, dtype=np.float32)
        
        # Dopasuj cechy
        matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)
        matches = matcher.knnMatch(des1, des2, k=2)
        
        # Filtruj dobre dopasowania (Lowe's ratio test)
        good_matches = []
        for match_pair in matches:
            if len(match_pair) == 2:
                m, n = match_pair
                if m.distance < 0.75 * n.distance:
                    good_matches.append(m)
        
        if len(good_matches) < 10:
            return image, np.eye(2, 3, dtype=np.float32)
        
        # Wyodrƒôbnij punkty
        src_pts = np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)
        dst_pts = np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)
        
        # Znajd≈∫ transformacjƒô afinicznƒÖ
        warp_matrix, inliers = cv2.estimateAffinePartial2D(dst_pts, src_pts)
        
        if warp_matrix is None:
            return image, np.eye(2, 3, dtype=np.float32)
        
        # Zastosuj transformacjƒô
        aligned = cv2.warpAffine(
            image, warp_matrix, (reference.shape[1], reference.shape[0])
        )
        
        return aligned, warp_matrix


class AnomalyDetector:
    """Wykrywa anomalie przez por√≥wnanie obraz√≥w"""
    
    def __init__(self, threshold: float = 25, min_area: int = 300, max_area: int = 50000,
                 background_threshold: int = 240):
        """
        Args:
            threshold: Pr√≥g r√≥≈ºnicy pikseli
            min_area: Minimalna powierzchnia anomalii (piksele)
            max_area: Maksymalna powierzchnia anomalii (piksele)
            background_threshold: Pr√≥g dla wykrywania bia≈Çego t≈Ça (0-255)
        """
        self.threshold = threshold
        self.min_area = min_area
        self.max_area = max_area
        self.background_threshold = background_threshold
    
    def detect_anomalies(self, reference: np.ndarray, image: np.ndarray,
                        use_ssim: bool = True, ignore_background: bool = True,
                        background_method: str = 'otsu') -> Dict:
        """
        Wykryj anomalie por√≥wnujƒÖc dwa obrazy
        
        Args:
            reference: Obraz wzorcowy (czysty)
            image: Obraz do sprawdzenia
            use_ssim: Czy u≈ºyƒá SSIM zamiast prostej r√≥≈ºnicy
            ignore_background: Czy ignorowaƒá bia≈Çe t≈Ço podczas detekcji
            background_method: Metoda wykrywania t≈Ça ('otsu', 'adaptive', 'threshold')
            
        Returns:
            S≈Çownik z wynikami detekcji
        """
        # Upewnij siƒô, ≈ºe obrazy majƒÖ ten sam rozmiar
        if reference.shape != image.shape:
            image = cv2.resize(image, (reference.shape[1], reference.shape[0]))
        
        # Wstƒôpne przetwarzanie
        ref_processed = self._preprocess(reference)
        img_processed = self._preprocess(image)
        
        if use_ssim:
            # SSIM - lepiej radzi sobie z niewielkimi r√≥≈ºnicami w jasno≈õci
            score, diff_map = ssim(ref_processed, img_processed, full=True)
            diff_map = (1 - diff_map) * 255
            diff_map = diff_map.astype(np.uint8)
        else:
            # Prosta r√≥≈ºnica bezwzglƒôdna
            diff_map = cv2.absdiff(ref_processed, img_processed)
        
        # Wykryj anomalie
        reference_for_mask = ref_processed if ignore_background else None
        anomalies = self._find_anomalies(diff_map, reference_for_mask)
        
        return {
            'difference_map': diff_map,
            'anomalies': anomalies,
            'anomaly_count': len(anomalies),
            'has_anomaly': len(anomalies) > 0,
            'ssim_score': score if use_ssim else None
        }
    
    def _preprocess(self, img: np.ndarray) -> np.ndarray:
        """Wstƒôpne przetwarzanie obrazu"""
        # Histogram equalization dla lepszego kontrastu
        img_eq = cv2.equalizeHist(img)
        
        # Denoising
        img_denoised = cv2.fastNlMeansDenoising(img_eq, h=10)
        
        return img_denoised
    
    def _create_background_mask(self, img: np.ndarray, method: str = 'otsu') -> np.ndarray:
        """
        Tworzy maskƒô t≈Ça dla obrazu RTG
        
        Args:
            img: Obraz w skali szaro≈õci
            method: Metoda wykrywania t≈Ça ('otsu', 'adaptive', 'threshold')
            
        Returns:
            Maska binarna (True dla obszar√≥w nie-t≈Ça)
        """
        if method == 'otsu':
            # Otsu thresholding - automatyczne wykrywanie progu
            _, binary = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
            # Inwersja - chcemy maskƒô obszar√≥w nie-t≈Ça
            mask = binary > 0
            
        elif method == 'adaptive':
            # Adaptive thresholding
            binary = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
                                         cv2.THRESH_BINARY, 11, 2)
            mask = binary > 0
            
        elif method == 'threshold':
            # Sta≈Çy pr√≥g dla bia≈Çego t≈Ça
            mask = img < self.background_threshold
            
        else:
            # Fallback - prosty pr√≥g
            mask = img < self.background_threshold
        
        # Operacje morfologiczne do oczyszczenia maski
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7, 7))
        mask_cleaned = cv2.morphologyEx(mask.astype(np.uint8), cv2.MORPH_CLOSE, kernel)
        mask_cleaned = cv2.morphologyEx(mask_cleaned, cv2.MORPH_OPEN, kernel)
        
        return mask_cleaned.astype(bool)
    
    def _find_anomalies(self, diff_map: np.ndarray, reference_img: np.ndarray = None) -> List[Dict]:
        """Znajd≈∫ regiony anomalii na mapie r√≥≈ºnic, ignorujƒÖc prawie bia≈Çe t≈Ço"""
        # Progowanie
        _, binary = cv2.threshold(diff_map, self.threshold, 255, cv2.THRESH_BINARY)
        
        # Maskowanie t≈Ça - ignoruj prawie bia≈Çe piksele (t≈Ço RTG)
        if reference_img is not None:
            # Utw√≥rz maskƒô obszar√≥w nie-t≈Ça (ROI - Region of Interest)
            roi_mask = self._create_background_mask(reference_img, method='otsu')
            
            # Zastosuj maskƒô - usu≈Ñ anomalie w obszarach bia≈Çego t≈Ça
            binary = binary & roi_mask.astype(np.uint8) * 255
            
            print(f"üéØ Zastosowano maskƒô ROI (obszary nie-t≈Ça)")
            print(f"   Procent obszaru ROI: {np.sum(roi_mask) / roi_mask.size * 100:.1f}%")
        else:
            print("‚ö†Ô∏è Brak obrazu referencyjnego - pomijam maskowanie t≈Ça")
        # Operacje morfologiczne
        kernel_open = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
        kernel_close = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (9, 9))
        
        binary = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel_open, iterations=2)
        binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel_close, iterations=2)
        
        # Znajd≈∫ kontury
        contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        anomalies = []
        for contour in contours:
            area = cv2.contourArea(contour)
            
            # Filtruj po obszarze
            if area < self.min_area or area > self.max_area:
                continue
            
            # Bounding box
            x, y, w, h = cv2.boundingRect(contour)
            
            # Filtruj po kszta≈Çcie (aspect ratio)
            aspect_ratio = w / h if h > 0 else 0
            if aspect_ratio > 10 or aspect_ratio < 0.1:
                continue
            
            # Solidno≈õƒá (solidity)
            hull = cv2.convexHull(contour)
            hull_area = cv2.contourArea(hull)
            solidity = area / hull_area if hull_area > 0 else 0
            
            if solidity < 0.3:  # Zbyt nieregularne
                continue
            
            anomalies.append({
                'bbox': (x, y, w, h),
                'area': area,
                'solidity': solidity,
                'aspect_ratio': aspect_ratio,
                'contour': contour
            })
        
        return anomalies


class AnomalyReportGenerator:
    """Generuje raporty z wykrytymi anomaliami"""
    
    @staticmethod
    def generate_report(original_img: np.ndarray, reference_img: np.ndarray,
                       aligned_img: np.ndarray, detection_result: Dict,
                       output_path: str, metadata: Dict = None,
                       reference_info: Dict = None) -> str:
        """
        Generuje kompletny raport wizualny
        
        Args:
            original_img: Oryginalny obraz do sprawdzenia
            reference_img: Dopasowany obraz wzorcowy
            aligned_img: Wyr√≥wnany obraz
            detection_result: Wyniki detekcji
            output_path: ≈öcie≈ºka do zapisu raportu
            metadata: Dodatkowe metadane
            
        Returns:
            ≈öcie≈ºka do wygenerowanego raportu
        """
        # Przygotuj wizualizacje
        annotated = AnomalyReportGenerator._draw_anomalies(
            original_img, detection_result['anomalies']
        )
        diff_colored = AnomalyReportGenerator._colorize_diff(
            detection_result['difference_map']
        )
        
        # Utw√≥rz grid z wizualizacjami
        report_img = AnomalyReportGenerator._create_report_grid(
            original_img, reference_img, aligned_img,
            diff_colored, annotated, detection_result, reference_info
        )
        
        # Zapisz raport obrazowy
        cv2.imwrite(output_path, report_img)
        
        # Generuj raport JSON
        json_path = output_path.rsplit('.', 1)[0] + '_report.json'
        AnomalyReportGenerator._save_json_report(
            json_path, detection_result, metadata
        )
        
        return output_path
    
    @staticmethod
    def _draw_anomalies(img: np.ndarray, anomalies: List[Dict]) -> np.ndarray:
        """Rysuje wykryte anomalie na obrazie"""
        # Konwertuj do BGR dla kolorowych adnotacji
        if len(img.shape) == 2:
            annotated = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)
        else:
            annotated = img.copy()
        
        for i, anomaly in enumerate(anomalies):
            x, y, w, h = anomaly['bbox']
            
            # Rysuj prostokƒÖt - zawsze czerwony
            cv2.rectangle(annotated, (x, y), (x+w, y+h), (0, 0, 255), 3)
            
            # Rysuj markery w rogach - tak≈ºe czerwone (wszystkie rogi)
            corner_size = 15
            # Lewy g√≥rny r√≥g
            cv2.line(annotated, (x, y), (x + corner_size, y), (0, 0, 255), 4)
            cv2.line(annotated, (x, y), (x, y + corner_size), (0, 0, 255), 4)
            # Prawy g√≥rny r√≥g
            cv2.line(annotated, (x+w, y), (x+w - corner_size, y), (0, 0, 255), 4)
            cv2.line(annotated, (x+w, y), (x+w, y + corner_size), (0, 0, 255), 4)
            # Lewy dolny r√≥g
            cv2.line(annotated, (x, y+h), (x + corner_size, y+h), (0, 0, 255), 4)
            cv2.line(annotated, (x, y+h), (x, y+h - corner_size), (0, 0, 255), 4)
            # Prawy dolny r√≥g
            cv2.line(annotated, (x+w, y+h), (x+w - corner_size, y+h), (0, 0, 255), 4)
            cv2.line(annotated, (x+w, y+h), (x+w, y+h - corner_size), (0, 0, 255), 4)
            
            # Etykieta - czerwone t≈Ço, bia≈Çy tekst
            label = f"A{i+1}: {anomaly['area']:.0f}px"
            cv2.putText(annotated, label, (x, y-10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
        
        return annotated
    
    @staticmethod
    def _colorize_diff(diff_map: np.ndarray) -> np.ndarray:
        """Koloruje mapƒô r√≥≈ºnic dla lepszej wizualizacji"""
        # Normalizuj do 0-255
        diff_normalized = cv2.normalize(diff_map, None, 0, 255, cv2.NORM_MINMAX)
        
        # Zastosuj kolorowƒÖ mapƒô (heatmap)
        colored = cv2.applyColorMap(diff_normalized.astype(np.uint8), cv2.COLORMAP_JET)
        
        return colored
    
    @staticmethod
    def _create_report_grid(original: np.ndarray, reference: np.ndarray,
                           aligned: np.ndarray, diff_colored: np.ndarray,
                           annotated: np.ndarray, detection_result: Dict,
                           reference_info: Dict = None) -> np.ndarray:
        """Tworzy grid z wszystkimi wizualizacjami"""
        # Konwertuj grayscale do BGR je≈õli potrzeba
        def to_bgr(img):
            if len(img.shape) == 2:
                return cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)
            return img
        
        original = to_bgr(original)
        reference = to_bgr(reference)
        aligned = to_bgr(aligned)
        annotated = to_bgr(annotated)
        
        # Upewnij siƒô, ≈ºe wszystkie majƒÖ ten sam rozmiar
        h, w = original.shape[:2]
        reference = cv2.resize(reference, (w, h))
        aligned = cv2.resize(aligned, (w, h))
        diff_colored = cv2.resize(diff_colored, (w, h))
        annotated = cv2.resize(annotated, (w, h))
        
        # Dodaj etykiety z informacjami o dopasowaniu
        def add_label(img, text, color=(255, 255, 255)):
            labeled = img.copy()
            cv2.rectangle(labeled, (0, 0), (w, 50), (0, 0, 0), -1)
            cv2.putText(labeled, text, (10, 35),
                       cv2.FONT_HERSHEY_SIMPLEX, 1.0, color, 2)
            return labeled
        
        # Przygotuj etykiety z dodatkowymi informacjami
        reference_label = "Obraz wzorcowy"
        if reference_info:
            ref_name = reference_info.get('name', 'nieznany')
            ref_similarity = reference_info.get('similarity', 0)
            ref_source = reference_info.get('source', 'original')
            source_emoji = "üîß" if ref_source == 'processed' else "üìÅ"
            reference_label = f"{source_emoji} {ref_name[:20]} ({ref_similarity:.1%})"
        
        original = add_label(original, "Obraz testowy")
        reference = add_label(reference, reference_label, (0, 255, 255))  # Cyan dla wyr√≥≈ºnienia
        aligned = add_label(aligned, "Wyrownany")
        diff_colored = add_label(diff_colored, "Mapa roznic (heatmap)")
        annotated = add_label(annotated, f"Wykryte anomalie: {len(detection_result['anomalies'])}", 
                            (0, 255, 0) if detection_result['has_anomaly'] else (255, 255, 255))
        
        # Grid 2x3 - lepsze u≈Ço≈ºenie
        row1 = np.hstack([original, reference, aligned])
        row2 = np.hstack([diff_colored, annotated, np.zeros_like(annotated)])  # Trzecia kolumna pusta
        
        grid = np.vstack([row1, row2])
        
        # Dodaj szczeg√≥≈Çowe podsumowanie na dole
        summary_height = 120
        summary = np.zeros((summary_height, grid.shape[1], 3), dtype=np.uint8)
        
        anomaly_count = len(detection_result['anomalies'])
        status = "ANOMALIA WYKRYTA!" if detection_result['has_anomaly'] else "BRAK ANOMALII"
        status_color = (0, 0, 255) if detection_result['has_anomaly'] else (0, 255, 0)
        
        # Linia 1: Status
        cv2.putText(summary, status, (20, 30),
                   cv2.FONT_HERSHEY_SIMPLEX, 1.2, status_color, 3)
        
        # Linia 2: Liczba anomalii
        cv2.putText(summary, f"Liczba anomalii: {anomaly_count}", (20, 60),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
        
        # Linia 3: Informacje o dopasowaniu
        if reference_info:
            match_text = f"Dopasowanie: {reference_info.get('name', 'nieznany')[:30]}"
            cv2.putText(summary, match_text, (20, 90),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
        
        # Po prawej: SSIM
        if detection_result.get('ssim_score') is not None:
            cv2.putText(summary, f"SSIM: {detection_result['ssim_score']:.4f}", 
                       (grid.shape[1] - 200, 40),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
        
        # Po prawej: Podobie≈Ñstwo
        if reference_info:
            similarity_text = f"Podobienstwo: {reference_info.get('similarity', 0):.1%}"
            cv2.putText(summary, similarity_text,
                       (grid.shape[1] - 200, 70),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
        
        final = np.vstack([grid, summary])
        
        return final
    
    @staticmethod
    def _save_json_report(json_path: str, detection_result: Dict, metadata: Dict = None):
        """Zapisuje raport w formacie JSON"""
        report = {
            'timestamp': datetime.now().isoformat(),
            'has_anomaly': detection_result['has_anomaly'],
            'anomaly_count': detection_result['anomaly_count'],
            'ssim_score': detection_result.get('ssim_score'),
            'anomalies': []
        }
        
        for i, anomaly in enumerate(detection_result['anomalies']):
            report['anomalies'].append({
                'id': i + 1,
                'bbox': anomaly['bbox'],
                'area': float(anomaly['area']),
                'solidity': float(anomaly['solidity']),
                'aspect_ratio': float(anomaly['aspect_ratio'])
            })
        
        if metadata:
            report['metadata'] = metadata
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)


class RTGAnomalySystem:
    """G≈Ç√≥wny system detekcji anomalii RTG"""
    
    def __init__(self, reference_dir: str, output_dir: str = 'anomaly_reports', 
                 processed_dir: str = None):
        """
        Args:
            reference_dir: Katalog z obrazami wzorcowymi (czystymi)
            output_dir: Katalog do zapisywania raport√≥w
            processed_dir: Katalog z przetworzonymi obrazami (opcjonalny)
        """
        # Automatycznie dodaj katalog z przetworzonymi obrazami je≈õli nie podano
        if processed_dir is None:
            processed_dir = 'data-processing/processed_clean_data'
        
        self.matcher = ImageMatcher(reference_dir, processed_dir)
        self.aligner = ImageAligner()
        self.detector = AnomalyDetector()
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        print(f"üöÄ System detekcji anomalii RTG zainicjalizowany")
        print(f"üìÇ Obrazy wzorcowe: {reference_dir}")
        if processed_dir:
            print(f"üîß Przetworzone: {processed_dir}")
        print(f"üìÇ Raporty: {output_dir}")
    
    def process_image(self, image_path: str, use_alignment: bool = True,
                     use_ssim: bool = True, save_report: bool = True,
                     ignore_background: bool = True) -> Dict:
        """
        Przetw√≥rz obraz i wykryj anomalie
        
        Args:
            image_path: ≈öcie≈ºka do obrazu do sprawdzenia
            use_alignment: Czy wyr√≥wnywaƒá obrazy
            use_ssim: Czy u≈ºyƒá SSIM
            save_report: Czy zapisaƒá raport
            ignore_background: Czy ignorowaƒá bia≈Çe t≈Ço podczas detekcji
            
        Returns:
            S≈Çownik z wynikami analizy
        """
        print(f"\nüîç Przetwarzanie: {image_path}")
        
        # Wczytaj obraz
        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
        if img is None:
            raise ValueError(f"Nie mo≈ºna wczytaƒá obrazu: {image_path}")
        
        # Znajd≈∫ najbardziej podobny obraz wzorcowy
        print("üîé Szukanie najbardziej podobnego obrazu wzorcowego...")
        matches = self.matcher.find_best_match(img, top_k=1)
        
        if not matches:
            raise ValueError("Nie znaleziono obraz√≥w wzorcowych")
        
        best_match = matches[0]
        reference_img = best_match['image']
        similarity = best_match['similarity']
        
        print(f"‚úÖ Znaleziono dopasowanie: {best_match['path'].name}")
        print(f"   Podobie≈Ñstwo: {similarity:.2%}")
        
        # Wyr√≥wnaj obrazy
        if use_alignment:
            print("‚öôÔ∏è Wyr√≥wnywanie obraz√≥w...")
            try:
                # Spr√≥buj szybkiego wyr√≥wnania najpierw
                if reference_img.shape != img.shape:
                    print("üìê Dopasowywanie rozmiar√≥w...")
                    img_resized = cv2.resize(img, (reference_img.shape[1], reference_img.shape[0]))
                else:
                    img_resized = img
                
                # Sprawd≈∫ czy obrazy sƒÖ podobne - je≈õli bardzo podobne, pomi≈Ñ ECC
                similarity_quick = cv2.matchTemplate(reference_img.astype(np.float32), 
                                                   img_resized.astype(np.float32), 
                                                   cv2.TM_CCOEFF_NORMED)[0,0]
                
                if similarity_quick > 0.95:
                    print(f"üöÄ Obrazy bardzo podobne ({similarity_quick:.3f}), pomijam dok≈Çadne wyr√≥wnywanie")
                    aligned_img = img_resized
                    transform = np.eye(2, 3, dtype=np.float32)
                else:
                    print(f"üîß Podobie≈Ñstwo: {similarity_quick:.3f}, uruchamiam dok≈Çadne wyr√≥wnywanie...")
                    aligned_img, transform = self.aligner.align_images(reference_img, img_resized)
                    
            except Exception as e:
                print(f"‚ùå B≈ÇƒÖd podczas wyr√≥wnywania: {e}")
                print("üîÑ U≈ºywam podstawowego dopasowania rozmiaru...")
                aligned_img = cv2.resize(img, (reference_img.shape[1], reference_img.shape[0]))
                transform = None
        else:
            aligned_img = cv2.resize(img, (reference_img.shape[1], reference_img.shape[0]))
            transform = None
        
        # Wykryj anomalie
        print("üî¨ Wykrywanie anomalii...")
        detection_result = self.detector.detect_anomalies(
            reference_img, aligned_img, use_ssim=use_ssim, 
            ignore_background=ignore_background
        )
        
        print(f"{'‚ùå' if detection_result['has_anomaly'] else '‚úÖ'} "
              f"Wykryto {detection_result['anomaly_count']} anomalii")
        
        # Generuj raport
        if save_report:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            img_name = Path(image_path).stem
            report_path = self.output_dir / f"report_{img_name}_{timestamp}.png"
            
            print(f"üìä Generowanie raportu...")
            
            # Przygotuj informacje o dopasowaniu dla raportu
            reference_info = {
                'name': best_match['path'].name,
                'similarity': similarity,
                'source': best_match.get('source', 'original'),
                'path': str(best_match['path'])
            }
            
            AnomalyReportGenerator.generate_report(
                img, reference_img, aligned_img, detection_result,
                str(report_path),
                metadata={
                    'input_image': image_path,
                    'reference_image': str(best_match['path']),
                    'reference_source': best_match.get('source', 'original'),
                    'similarity': similarity,
                    'alignment_used': use_alignment,
                    'ssim_used': use_ssim
                },
                reference_info=reference_info
            )
            print(f"üíæ Raport zapisany: {report_path}")
        
        return {
            'has_anomaly': detection_result['has_anomaly'],
            'anomaly_count': detection_result['anomaly_count'],
            'anomalies': detection_result['anomalies'],
            'reference_match': str(best_match['path']),
            'similarity': similarity,
            'ssim_score': detection_result.get('ssim_score'),
            'report_path': str(report_path) if save_report else None
        }
    
    def batch_process(self, image_dir: str, pattern: str = '*.bmp') -> List[Dict]:
        """
        Przetwarzaj wiele obraz√≥w w partii
        
        Args:
            image_dir: Katalog z obrazami do sprawdzenia
            pattern: Wzorzec nazw plik√≥w
            
        Returns:
            Lista wynik√≥w dla ka≈ºdego obrazu
        """
        image_paths = list(Path(image_dir).rglob(pattern))
        print(f"\nüì¶ Przetwarzanie partiami: {len(image_paths)} obraz√≥w")
        
        results = []
        for img_path in image_paths:
            try:
                result = self.process_image(str(img_path))
                results.append(result)
            except Exception as e:
                print(f"‚ùå B≈ÇƒÖd przetwarzania {img_path}: {e}")
                results.append({'error': str(e), 'path': str(img_path)})
        
        # Podsumowanie
        anomaly_count = sum(1 for r in results if r.get('has_anomaly', False))
        print(f"\nüìà Podsumowanie przetwarzania partii:")
        print(f"   Przetworzono: {len(results)} obraz√≥w")
        print(f"   Z anomaliami: {anomaly_count}")
        print(f"   Bez anomalii: {len(results) - anomaly_count}")
        
        return results


# Funkcja pomocnicza do szybkiego u≈ºycia
def quick_detect(image_path: str, reference_dir: str = 'data/czyste',
                output_dir: str = 'anomaly_reports', 
                processed_dir: str = 'data-processing/processed_clean_data') -> Dict:
    """
    Szybka detekcja anomalii dla pojedynczego obrazu
    
    Args:
        image_path: ≈öcie≈ºka do obrazu
        reference_dir: Katalog z obrazami wzorcowymi
        output_dir: Katalog do zapisywania raport√≥w
        processed_dir: Katalog z przetworzonymi obrazami
        
    Returns:
        Wyniki detekcji
    """
    system = RTGAnomalySystem(reference_dir, output_dir, processed_dir)
    return system.process_image(image_path)


if __name__ == "__main__":
    # Przyk≈Çad u≈ºycia
    print("=" * 80)
    print("üî¨ System Detekcji Anomalii RTG")
    print("=" * 80)
    
    # Inicjalizuj system
    system = RTGAnomalySystem(
        reference_dir='data/czyste',
        output_dir='anomaly_reports',
        processed_dir='data-processing/processed_clean_data'
    )
    
    # Testuj na obrazach z anomaliami
    test_dir = 'data/brudne'
    if os.path.exists(test_dir):
        results = system.batch_process(test_dir, pattern='*.bmp')
        
        # Wy≈õwietl podsumowanie
        print("\n" + "=" * 80)
        print("üìä SZCZEG√ì≈ÅOWE WYNIKI")
        print("=" * 80)
        for i, result in enumerate(results, 1):
            if 'error' not in result:
                print(f"\n{i}. {'üî¥ ANOMALIA' if result['has_anomaly'] else 'üü¢ CZYSTE'}")
                print(f"   Wykryto: {result['anomaly_count']} anomalii")
                print(f"   Podobie≈Ñstwo do wzorca: {result['similarity']:.2%}")
                if result.get('ssim_score'):
                    print(f"   SSIM: {result['ssim_score']:.4f}")
    else:
        print(f"‚ö†Ô∏è Katalog testowy nie istnieje: {test_dir}")
        print("U≈ºyj: python anomaly_detector.py lub quick_detect('path/to/image.bmp')")
